{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd06aae137f62a355015b5dff5f2f5a619ab6dc93c11d6273a899106161481eae5c",
   "display_name": "Python 3.7.10 64-bit ('demoEnv': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "6aae137f62a355015b5dff5f2f5a619ab6dc93c11d6273a899106161481eae5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple('Episode', ['reward', 'steps'])\n",
    "Episode_Step = namedtuple('Episode_Step', ['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHot(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHot, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0,1.0,shape=(env.observation_space.n,), dtype=np.float32)\n",
    "    def observation(self, observation):\n",
    "        obs = np.copy(self.observation_space.low)\n",
    "        obs[observation] = 1.0\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(nn.Module):\n",
    "    def __init__(self,in_units, hidden_units, out_units):\n",
    "        super(Player,self).__init__()\n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.Linear(in_features=in_units, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=out_units),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_UNITS = 16\n",
    "HIDDEN_UNITS = 128\n",
    "OUT_UNITS = 4\n",
    "BATCH_SIZE = 100\n",
    "PERCENTILE = 30\n",
    "GAMMA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(env, policy):\n",
    "    batch = []\n",
    "    \n",
    "    sample_step = []\n",
    "    sample_reward = 0.0\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "   \n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(policy(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p = act_probs)\n",
    "        \n",
    "        \n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        sample_reward += reward\n",
    "        sample_step.append(Episode_Step(observation = obs, action = action))\n",
    "        \n",
    "        if done:\n",
    "            batch.append(Episode(steps = sample_step, reward  = sample_reward))\n",
    "            \n",
    "            sample_step = []\n",
    "            sample_reward = 0.0\n",
    "           \n",
    "            next_obs = env.reset()\n",
    "            \n",
    "            if len(batch) == BATCH_SIZE:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    disc_rewards = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), batch))\n",
    "    reward_bound = np.percentile(disc_rewards, percentile)\n",
    "    # reward_mean = float(np.mean(rewards))\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch = []\n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        if discounted_reward > reward_bound:\n",
    "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "            elite_batch.append(example)\n",
    "\n",
    "    \n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return elite_batch, train_obs_v, train_act_v, reward_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = gym.logger\n",
    "log.set_level(gym.logger.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    env = DiscreteOneHot(gym.make(\"FrozenLake-v0\"))\n",
    "    player = Player(IN_UNITS, HIDDEN_UNITS, OUT_UNITS)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    writer = SummaryWriter()\n",
    "    optimizer = optim.Adam(params=player.parameters(),lr=0.001)\n",
    "    full_batch = []\n",
    "    for ite, batch in enumerate(get_batch(env,player)):\n",
    "        reward_mean = float(np.mean(list(map(lambda s: s.reward, batch))))\n",
    "        full_batch, train_obs, train_act, reward_bound = filter_batch( full_batch+batch, PERCENTILE)\n",
    "        if not full_batch:\n",
    "            continue\n",
    "                \n",
    "        full_batch = full_batch[-500:]\n",
    "        # print(train_obs.shape)\n",
    "        optimizer.zero_grad()\n",
    "        actions = player(train_obs)\n",
    "        player_loss = loss(actions, train_act)\n",
    "        player_loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar(\"Loss\", player_loss, ite)\n",
    "        writer.add_scalar(\"Reward Mean\", reward_mean, ite)\n",
    "        writer.add_scalar(\"Reward Bound\", reward_bound, ite)\n",
    "        log.info(\"Iter %d: Loss=%.3f, Reward=%.3f\", ite,player_loss,reward_mean)\n",
    "        if reward_mean >=0.8:\n",
    "            writer.close()\n",
    "            env.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO: Making new env: FrozenLake-v0\n",
      "INFO: Iter 0: Loss=1.402, Reward=0.010\n",
      "INFO: Iter 1: Loss=1.393, Reward=0.010\n",
      "INFO: Iter 2: Loss=1.383, Reward=0.020\n",
      "INFO: Iter 3: Loss=1.386, Reward=0.020\n",
      "INFO: Iter 4: Loss=1.392, Reward=0.020\n",
      "INFO: Iter 5: Loss=1.389, Reward=0.010\n",
      "INFO: Iter 6: Loss=1.385, Reward=0.020\n",
      "INFO: Iter 7: Loss=1.383, Reward=0.020\n",
      "INFO: Iter 8: Loss=1.385, Reward=0.010\n",
      "INFO: Iter 9: Loss=1.384, Reward=0.000\n",
      "INFO: Iter 10: Loss=1.382, Reward=0.000\n",
      "INFO: Iter 11: Loss=1.378, Reward=0.050\n",
      "INFO: Iter 12: Loss=1.377, Reward=0.020\n",
      "INFO: Iter 13: Loss=1.378, Reward=0.010\n",
      "INFO: Iter 14: Loss=1.377, Reward=0.020\n",
      "INFO: Iter 15: Loss=1.376, Reward=0.000\n",
      "INFO: Iter 16: Loss=1.375, Reward=0.010\n",
      "INFO: Iter 17: Loss=1.374, Reward=0.030\n",
      "INFO: Iter 18: Loss=1.373, Reward=0.000\n",
      "INFO: Iter 19: Loss=1.373, Reward=0.030\n",
      "INFO: Iter 20: Loss=1.370, Reward=0.020\n",
      "INFO: Iter 21: Loss=1.372, Reward=0.030\n",
      "INFO: Iter 22: Loss=1.370, Reward=0.020\n",
      "INFO: Iter 23: Loss=1.369, Reward=0.000\n",
      "INFO: Iter 24: Loss=1.368, Reward=0.010\n",
      "INFO: Iter 25: Loss=1.369, Reward=0.010\n",
      "INFO: Iter 26: Loss=1.369, Reward=0.020\n",
      "INFO: Iter 27: Loss=1.368, Reward=0.030\n",
      "INFO: Iter 28: Loss=1.366, Reward=0.010\n",
      "INFO: Iter 29: Loss=1.364, Reward=0.020\n",
      "INFO: Iter 30: Loss=1.364, Reward=0.010\n",
      "INFO: Iter 31: Loss=1.364, Reward=0.020\n",
      "INFO: Iter 32: Loss=1.362, Reward=0.010\n",
      "INFO: Iter 33: Loss=1.361, Reward=0.010\n",
      "INFO: Iter 34: Loss=1.360, Reward=0.000\n",
      "INFO: Iter 35: Loss=1.360, Reward=0.020\n",
      "INFO: Iter 36: Loss=1.359, Reward=0.010\n",
      "INFO: Iter 37: Loss=1.358, Reward=0.060\n",
      "INFO: Iter 38: Loss=1.358, Reward=0.020\n",
      "INFO: Iter 39: Loss=1.357, Reward=0.010\n",
      "INFO: Iter 40: Loss=1.357, Reward=0.040\n",
      "INFO: Iter 41: Loss=1.356, Reward=0.030\n",
      "INFO: Iter 42: Loss=1.354, Reward=0.040\n",
      "INFO: Iter 43: Loss=1.353, Reward=0.000\n",
      "INFO: Iter 44: Loss=1.352, Reward=0.000\n",
      "INFO: Iter 45: Loss=1.351, Reward=0.030\n",
      "INFO: Iter 46: Loss=1.350, Reward=0.010\n",
      "INFO: Iter 47: Loss=1.348, Reward=0.030\n",
      "INFO: Iter 48: Loss=1.347, Reward=0.020\n",
      "INFO: Iter 49: Loss=1.347, Reward=0.040\n",
      "INFO: Iter 50: Loss=1.346, Reward=0.000\n",
      "INFO: Iter 51: Loss=1.345, Reward=0.010\n",
      "INFO: Iter 52: Loss=1.345, Reward=0.000\n",
      "INFO: Iter 53: Loss=1.343, Reward=0.050\n",
      "INFO: Iter 54: Loss=1.341, Reward=0.010\n",
      "INFO: Iter 55: Loss=1.340, Reward=0.050\n",
      "INFO: Iter 56: Loss=1.338, Reward=0.010\n",
      "INFO: Iter 57: Loss=1.338, Reward=0.000\n",
      "INFO: Iter 58: Loss=1.336, Reward=0.030\n",
      "INFO: Iter 59: Loss=1.335, Reward=0.010\n",
      "INFO: Iter 60: Loss=1.334, Reward=0.000\n",
      "INFO: Iter 61: Loss=1.334, Reward=0.000\n",
      "INFO: Iter 62: Loss=1.332, Reward=0.040\n",
      "INFO: Iter 63: Loss=1.331, Reward=0.000\n",
      "INFO: Iter 64: Loss=1.330, Reward=0.050\n",
      "INFO: Iter 65: Loss=1.329, Reward=0.000\n",
      "INFO: Iter 66: Loss=1.328, Reward=0.040\n",
      "INFO: Iter 67: Loss=1.327, Reward=0.010\n",
      "INFO: Iter 68: Loss=1.326, Reward=0.000\n",
      "INFO: Iter 69: Loss=1.325, Reward=0.010\n",
      "INFO: Iter 70: Loss=1.322, Reward=0.060\n",
      "INFO: Iter 71: Loss=1.322, Reward=0.040\n",
      "INFO: Iter 72: Loss=1.321, Reward=0.010\n",
      "INFO: Iter 73: Loss=1.320, Reward=0.020\n",
      "INFO: Iter 74: Loss=1.318, Reward=0.020\n",
      "INFO: Iter 75: Loss=1.317, Reward=0.010\n",
      "INFO: Iter 76: Loss=1.318, Reward=0.030\n",
      "INFO: Iter 77: Loss=1.316, Reward=0.010\n",
      "INFO: Iter 78: Loss=1.315, Reward=0.030\n",
      "INFO: Iter 79: Loss=1.314, Reward=0.060\n",
      "INFO: Iter 80: Loss=1.313, Reward=0.040\n",
      "INFO: Iter 81: Loss=1.312, Reward=0.060\n",
      "INFO: Iter 82: Loss=1.311, Reward=0.030\n",
      "INFO: Iter 83: Loss=1.311, Reward=0.040\n",
      "INFO: Iter 84: Loss=1.309, Reward=0.050\n",
      "INFO: Iter 85: Loss=1.309, Reward=0.040\n",
      "INFO: Iter 86: Loss=1.307, Reward=0.030\n",
      "INFO: Iter 87: Loss=1.306, Reward=0.010\n",
      "INFO: Iter 88: Loss=1.306, Reward=0.020\n",
      "INFO: Iter 89: Loss=1.305, Reward=0.020\n",
      "INFO: Iter 90: Loss=1.304, Reward=0.010\n",
      "INFO: Iter 91: Loss=1.304, Reward=0.010\n",
      "INFO: Iter 92: Loss=1.304, Reward=0.020\n",
      "INFO: Iter 93: Loss=1.304, Reward=0.060\n",
      "INFO: Iter 94: Loss=1.302, Reward=0.030\n",
      "INFO: Iter 95: Loss=1.300, Reward=0.030\n",
      "INFO: Iter 96: Loss=1.299, Reward=0.060\n",
      "INFO: Iter 97: Loss=1.299, Reward=0.040\n",
      "INFO: Iter 98: Loss=1.297, Reward=0.020\n",
      "INFO: Iter 99: Loss=1.296, Reward=0.030\n",
      "INFO: Iter 100: Loss=1.295, Reward=0.040\n",
      "INFO: Iter 101: Loss=1.294, Reward=0.010\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = get_batch(env, player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = next(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n"
     ]
    }
   ],
   "source": [
    "for b in b1:\n",
    "    print(b.reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = nn.Softmax(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.1388,  0.0958, -0.0763,  0.0586]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "act = player(torch.tensor([obs]))\n",
    "act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2712, 0.2598, 0.2187, 0.2503]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "sm(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step()"
   ]
  }
 ]
}